{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "os.chdir(\"AudioCLIP/demo/\")\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}/..'))\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Partial-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
    "\n",
    "aclp = AudioCLIP(pretrained=f'../assets/{MODEL_FILENAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_transforms = ToTensor1D()\n",
    "\n",
    "image_transforms = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Resize(IMAGE_SIZE, interpolation=Image.BICUBIC),\n",
    "    tv.transforms.CenterCrop(IMAGE_SIZE),\n",
    "    tv.transforms.Normalize(IMAGE_MEAN, IMAGE_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAudioEmbeddings(sourcePath, destPath, instrument):\n",
    "    audioPaths = glob.glob(f\"{sourcePath}/{instrument}/*.wav\")\n",
    "    audio = []\n",
    "\n",
    "    for path in tqdm(audioPaths, desc=f\"{{{instrument}}}\"):\n",
    "        track, _ = librosa.load(path, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "        spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
    "        spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
    "        pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "        audio.append((track, pow_spec))\n",
    "  \n",
    "\n",
    "    audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
    "    ((audio_features, _, _), _), _ = aclp(audio=audio)\n",
    "    audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
    "    torch.save(audio_features, f\"{destPath}/{instrument}_audio_embeddings.pt\")\n",
    "\n",
    "\n",
    "def getImageEmbeddings(sourcePath, destPath, instrument):\n",
    "    imagePath = glob.glob(f\"{sourcePath}/{instrument}/*.wav\")\n",
    "    images = []\n",
    "\n",
    "    for path in tqdm(imagePath, desc=f\"{{{instrument}}}\"):\n",
    "        with open(path, 'rb') as jpg:\n",
    "            image = simplejpeg.decode_jpeg(jpg.read())\n",
    "            images.append(image) \n",
    "  \n",
    "\n",
    "    images = torch.stack([image_transforms(image) for image in images])  \n",
    "    ((_, image_features, _), _), _ = aclp(image=images)\n",
    "    image_features = image_features / torch.linalg.norm(image_features, dim=-1, keepdim=True)  \n",
    "    torch.save(image_features, f\"{destPath}/{instrument}_image_embeddings.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bassoon', 'cello', 'clarinet', 'double_bass', 'flute', 'horn', 'oboe', 'sax', 'trombone', 'trumpet', 'tuba', 'viola', 'violin']\n"
     ]
    }
   ],
   "source": [
    "dataPathAudio = f\"../../Data/SubURMPExtendedAudio/clean/validation\"\n",
    "embeddingPathAudio = f\"../../Embeddings/audio/subURMPExtendedAudio/validation\"\n",
    "\n",
    "dataPathImage = f\"../../Data/SubURMPClean/images/validation/\"\n",
    "embeddingPathImage = f\"../../Embeddings/images/validation/\"\n",
    "\n",
    "instrumentNames = os.listdir(dataPathAudio)\n",
    "instrumentNames = [name for name in instrumentNames if name[0] != '.']\n",
    "instrumentNames.sort()\n",
    "\n",
    "print(instrumentNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrumentNames = [\"sax\", \"trombone\", \"trumpet\", \"tuba\", \"viola\", \"violin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{sax}: 100%|██████████| 17/17 [00:03<00:00,  5.55it/s]\n",
      "{trombone}: 100%|██████████| 16/16 [00:02<00:00,  5.74it/s]\n",
      "{trumpet}: 100%|██████████| 10/10 [00:01<00:00,  5.74it/s]\n",
      "{tuba}: 100%|██████████| 10/10 [00:01<00:00,  5.69it/s]\n",
      "{viola}: 100%|██████████| 9/9 [00:01<00:00,  5.66it/s]\n",
      "{violin}: 100%|██████████| 18/18 [00:03<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for instrument in instrumentNames:\n",
    "    getAudioEmbeddings(dataPathAudio, embeddingPathAudio, instrument)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5aab599269adaaf86474dbe31d53360313243c527f071880033aa7bf61d9bed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
